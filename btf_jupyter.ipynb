{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import tweepy\n",
    "from tweepy.auth import OAuthHandler\n",
    "import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup database, credentials, and query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two `sqlite3` databases to store the tweets.\n",
    "\n",
    "We will use the Search API to go back in time and save data to `histtweets.db`, and the Streaming API to grab tweets in realtime to `livetweets.db`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create first database and table (run once)\n",
    "\n",
    "conn = sqlite3.connect(\"livetweets.db\")\n",
    "c = conn.cursor()\n",
    "c.execute(\"\"\"CREATE TABLE tweets (\n",
    "id TEXT,\n",
    "created_at TEXT,\n",
    "author TEXT,\n",
    "author_location TEXT,\n",
    "author_followers INT,\n",
    "author_friends INT,\n",
    "hashtags TEXT,\n",
    "tweet TEXT,\n",
    "in_reply_to TEXT,\n",
    "lang TEXT,\n",
    "method TEXT,\n",
    "UNIQUE(id))\n",
    "\"\"\")\n",
    "\n",
    "print(\"Database created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create second database and table (run once)\n",
    "\n",
    "conn = sqlite3.connect(\"histtweets.db\")\n",
    "c = conn.cursor()\n",
    "c.execute(\"\"\"CREATE TABLE tweets (\n",
    "id TEXT,\n",
    "created_at TEXT,\n",
    "author TEXT,\n",
    "author_location TEXT,\n",
    "author_followers INT,\n",
    "author_friends INT,\n",
    "hashtags TEXT,\n",
    "tweet TEXT,\n",
    "in_reply_to TEXT,\n",
    "lang TEXT,\n",
    "method TEXT,\n",
    "UNIQUE(id))\n",
    "\"\"\")\n",
    "\n",
    "print(\"Database created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Twitter credentials and authorise with the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token = \"\"\n",
    "access_secret = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both API searches and streams can be quite refined based on user accounts and so on. We focus here on searching/streaming for a set of **keywords**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter (manually) a Python formatted list of keywords\n",
    "keywords = ['fire', 'power']\n",
    "\n",
    "# Generate queries formatted for the Search and Streaming API respectively.\n",
    "searchquery = \" OR \".join(keywords)\n",
    "streamingquery = keywords # same format\n",
    "\n",
    "print(searchquery)\n",
    "print(streamingquery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the historical download, we go 2 days back in time. This means getting tweets from today (up until the point of launching the search), yesterday (1), and the day before yesterday (2). The API will return tweets from all hours of these two days regardless of what time during today the script is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = str(datetime.date.today() - datetime.timedelta(days=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, search backwards\n",
    "\n",
    "This is a linear notebook version of this method. In practice, it is better to run the backwards and forwards search at the same time, to get as many tweets as possible. For the sake of illustration, we run them here one at a time. First, the backwards search. It is probably good to re-run this search an extra time before moving to the next step, to get any new tweets that have been posted while the search was running. \n",
    "\n",
    "The code below connects to the Search API and starts retrieving tweets.\n",
    "\n",
    "Tweets will be downloaded in reverse order, starting from now and going back in time. This means that in many cases we may not want the full 2 days back, but can break the process manually once we have reached the point in time from where we want data. \n",
    "\n",
    "If there are many results, the script will pause from time to time due to rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('histtweets.db')\n",
    "\n",
    "c = tweepy.Cursor(api.search,\n",
    "                  q=searchquery,\n",
    "                  since = start,\n",
    "                  wait_on_rate_limit = True,\n",
    "                  wait_on_rate_limit_notify=True).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Searching backwards since \" + str(starttime))\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        tweet = c.next()\n",
    "        json_tweet = tweet._json     \n",
    "        \n",
    "        try:  \n",
    "            tweet_id = json_tweet['id_str']\n",
    "            tweet_created_at = json_tweet['created_at']\n",
    "            tweet_author = json_tweet['user']['screen_name']           \n",
    "            author_location = json_tweet['user']['location']\n",
    "            author_followers = json_tweet['user']['followers_count'] if not None else 0\n",
    "            author_friends = json_tweet['user']['friends_count'] if not None else 0\n",
    "            hashtags = json_tweet['entities']['hashtags']\n",
    "            tweet_hashtags = []\n",
    "            for hashtag in hashtags:\n",
    "                tweet_hashtags.append(\"#\" + str(hashtag['text']))\n",
    "            tweet_hashtags = \",\".join(tweet_hashtags)\n",
    "            tweet_text = json_tweet['text']\n",
    "            to_whom = json_tweet['in_reply_to_screen_name']\n",
    "            tweet_lang = json_tweet['lang']\n",
    "            conn.execute('INSERT INTO tweets (id, created_at, author, author_location, author_followers, author_friends, hashtags, tweet, in_reply_to, lang, method) VALUES (?,?,?,?,?,?,?,?,?,?,?)', (tweet_id, tweet_created_at, tweet_author, author_location, author_followers, author_friends, tweet_hashtags, tweet_text, to_whom, tweet_lang, \"StreamingAPI\"))\n",
    "            conn.commit()\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"select * from tweets\")\n",
    "            r = cursor.fetchall() \n",
    "            print(\"\\rTweet from \" + str(tweet.created_at) + \" (\" + str(len(r)) +\")              \", end='')     \n",
    "        except KeyError:\n",
    "            print(\"Key Error\")\n",
    "\n",
    "        except sqlite3.IntegrityError:\n",
    "            print(\"\\rAlready in database, continuing...\", end=\"\")\n",
    "\n",
    "    except IOError:\n",
    "        time.sleep(60*5)\n",
    "        continue\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "print(\"Done!\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of entries in the database\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"select * from tweets\")\n",
    "hist = str(len(cursor.fetchall()))\n",
    "print(\"Historical tweets: \" + hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second, stream forward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, collect data in real time by starting a listener for the Streaming API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('livetweets.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a twitter listener\n",
    "\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        data = json.loads(data)\n",
    "        tweet_id = int(data['id'])\n",
    "        tweet_created_at = data['created_at']\n",
    "        tweet_author = data['user']['screen_name']\n",
    "        author_location = data['user']['location']\n",
    "        author_followers = data['user']['followers_count'] if not None else 0\n",
    "        author_friends = data['user']['friends_count'] if not None else 0\n",
    "        hashtags = data['entities']['hashtags']\n",
    "        tweet_hashtags = []\n",
    "        for hashtag in hashtags:\n",
    "            tweet_hashtags.append(\"#\" + str(hashtag['text']))\n",
    "        tweet_hashtags = \",\".join(tweet_hashtags)\n",
    "        tweet_text = data['text']\n",
    "        in_reply_to = data['in_reply_to_screen_name']\n",
    "        tweet_lang = data['lang']\n",
    "        conn.execute('INSERT INTO tweets (id, created_at, author, author_location, author_followers, author_friends, hashtags, tweet, in_reply_to, lang, method) VALUES (?,?,?,?,?,?,?,?,?,?,?)', (tweet_id, tweet_created_at, tweet_author, author_location, author_followers, author_friends, tweet_hashtags, tweet_text, in_reply_to, tweet_lang, \"StreamingAPI\"))\n",
    "        conn.commit()\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"select * from tweets\")\n",
    "        r = cursor.fetchall() \n",
    "        print(\"\\rTweet from \" + str(tweet_created_at[:-10]) + \" (\" + str(len(r)) +\")              \", end='')\n",
    "            \n",
    "# Create a stream\n",
    "twitter_stream = tweepy.Stream(auth, MyStreamListener())\n",
    "\n",
    "# Start streaming and check for errors\n",
    "starttime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Running since \" + str(starttime))\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        twitter_stream.filter(track=streamingquery)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    except sqlite3.IntegrityError: # skip duplicate tweet ids\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the two databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get connections to the databases\n",
    "db_a = sqlite3.connect('livetweets.db')\n",
    "db_b = sqlite3.connect('histtweets.db')\n",
    "\n",
    "# Get the contents of a table\n",
    "b_cursor = db_b.cursor()\n",
    "b_cursor.execute('SELECT * FROM tweets')\n",
    "output = b_cursor.fetchall()   # Returns the results as a list.\n",
    "\n",
    "# Insert those contents into another table.\n",
    "a_cursor = db_a.cursor()\n",
    "for row in output:\n",
    "    try:\n",
    "        a_cursor.execute('INSERT INTO tweets VALUES (?,?,?,?,?,?,?,?,?,?,?)', row)\n",
    "    except sqlite3.IntegrityError: # skip duplicate tweet ids\n",
    "        pass\n",
    "\n",
    "# Cleanup\n",
    "db_a.commit()\n",
    "a_cursor.close()\n",
    "b_cursor.close()\n",
    "\n",
    "# Rename the merged db, and delete the other\n",
    "os.rename('livetweets.db', 'tweets.db')\n",
    "os.remove('histtweets.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the data\n",
    "\n",
    "It is worth noting that for the `in_reply_to`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sqlite query results into a pandas DataFrame\n",
    "conn = sqlite3.connect(\"tweets.db\")\n",
    "tweets_df = pd.read_sql_query(\"SELECT * from tweets\", conn)\n",
    "tweets_df = df.sort_values(by=\"created_at\")\n",
    "tweets_df = tweets_df.replace({'\\n': ' '}, regex=True) # remove linebreaks in the dataframe\n",
    "tweets_df = tweets_df.replace({'\\t': ' '}, regex=True) # remove tabs in the dataframe\n",
    "tweets_df = tweets_df.replace({'\\r': ' '}, regex=True) # remove carriage return in the dataframe\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
